{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Майнор по Анализу Данных, Группа ИАД-2\n",
    "## Домашнее задание №2: Классификация текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr\\>\n",
    "В задании вы будете решать задачу бинарной классификации текстов. Вы познакомитесь с основными инструментами sklearn, необходимыми для обработки текстов. Перед применением методов sklearn внимательно читайте документацию к ним: это полезно и помогает делать меньше ошибок.\n",
    "\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 1 мая 2017, 9:00 <br\\>\n",
    "\n",
    "При отправлении ДЗ на почту `hse.minor.dm+X@gmail.com`, X = 3 или 4 (ИАД 3 или ИАД4), указывайте фамилию в названии файла, а тему письма оформляйте в следующем виде:<br\\>\n",
    "** [HW2, ИАД-X] Фамилия Имя **<br\\>\n",
    "\n",
    "Сопровождайте ваш код изображеними, комментариями и выводами. <br\\>\n",
    "Имейте ввиду, что на некоторые задачи нет единственного верного и полного ответа. Чем больше информации вы сможете извлечь, аргументированных выводов сформулировать, тем лучше.\n",
    "__Старайтесь не копировать похожие участки кода. Везде, где это возможно, оформляйте код в функцию.__\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Чтобы узнать свой вариант, введите Вашу фамилию на русском языке в соответвующее поле ниже и запустите ячейку:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваш вариант -  2\n"
     ]
    }
   ],
   "source": [
    "name = \"БобровскихГлеб\" # Ваши ФамилияИмя\n",
    "\n",
    "alp = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "w = [4, 42, 21, 21, 34,  1, 44, 26, 18, 43, 38, 26, 18, 43,  3, 49, 45,\n",
    "        7, 42, 25,  4,  9, 36, 33, 31, 29,  5, 31,  4, 19, 24, 27, 33]\n",
    "d = dict(zip(alp, w))\n",
    "variant =  sum([d[el] for el in name.lower()]) % 2 + 1\n",
    "print(\"Ваш вариант - \", variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Варианты</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В зависимости от варианта нужно будет научиться определять...\n",
    "\n",
    "**1.** ...является ли SMS сообщение спамом? \n",
    "* Зайдите на [страничку с данными](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) на сайте репозитория UCI.\n",
    "* Нажмите на «Data Folder», скачайте и распакуйте архив.\n",
    "* Открыть SMSSpamCollection можно с помощью pd.read_csv, указав `sep='\\t'`.\n",
    "\n",
    "**2.** ...положительна или отрицательна рецензия на фильм?\n",
    "* Зайдите на [страничку с данными](http://www.cs.cornell.edu/people/pabo/movie-review-data/) на сайте Корнельского университета.\n",
    "* Нажмите на «polarity dataset v2.0» и распакуйте архив. \n",
    "* Каждый текстовый файл соответствует одной рецензии. Вам придётся [построить список всех файлов в папке](http://stackoverflow.com/questions/3207219/how-to-list-all-files-of-a-directory), а затем последовательно открыть их и прочитать тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_text(s):\n",
    "    # Removes all characters from string except letters and digits and convert letters to lowercase\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", s.lower())\n",
    "#explain regular expression:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '/home/bobrg/anaconda3/txt_sentoken/pos'\n",
    "onlyfiles = []\n",
    "onlyfiles = [convert_text(open(join(mypath, f), 'r').read())for f in listdir(mypath)\n",
    "             if isfile(join(mypath, f))]\n",
    "labels = [1] * len(onlyfiles)\n",
    "mypath = '/home/bobrg/anaconda3/txt_sentoken/neg'\n",
    "onlyfiles.extend([convert_text(open(join(mypath, f), 'r').read()) for f in listdir(mypath)\n",
    "                               if isfile(join(mypath, f))])\n",
    "labels.extend([0] * (len(onlyfiles) - len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1\n",
    "#### Классификация текстовых сообщений (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Загрузите исходные данные --- список текстов и список соответствующих им меток</li>\n",
    "    <li>Разбейте объекты на обучающее (train) и тестовое подможества (test) в пропорции 7:3</li>\n",
    "    <li>Переведите текстовые данные в векторный вид. Для этого воcпользуйтесь средствами sklearn для конвертации текста в векторы TF-IDF (настраивать только на обучающем подмножестве, n-gram=1, слова приведите в нижний регистр)</li>\n",
    "    <li>Постройте на обучающем подмножестве следующие модели классификации:\n",
    "        <ul>\n",
    "            <li>K-ближайших соседей ($n=5$)</li>\n",
    "            <li>Логистическая регрессия ($C=1$)</li>\n",
    "            <li>Мультиномиальный наивный Байес ($\\alpha=1$)</li> \n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Определите качество классификации (по доле правильных классификаций) на тестовом подмножестве</li>\n",
    "    <li>Определите с помощью timeit время обучения и предсказания (на тестовом подмножестве) </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 12345 # для всех объектов/методов/моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Параметры логистической регрессии\n",
    "```\n",
    "penalty='l2'\n",
    "fit_intercept=True\n",
    "max_iter=100\n",
    "C=1\n",
    "solver=\"lbfgs\"\n",
    "random_state=12345\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(onlyfiles, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) / len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(ngram_range=(1, 1))\n",
    "df = pd.DataFrame(data=count_vec.fit_transform(X_train).toarray(),\n",
    "                  columns=count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(use_idf=False)\n",
    "df_train = pd.DataFrame(data=transformer.fit_transform(np.asarray(df)).toarray(),\n",
    "                       columns=count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to make transformation:\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer df_train = TfidfVectorizer(lowercase=True,        ngram_range=(1,1)).fit_transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0009f</th>\n",
       "      <th>007</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>100m</th>\n",
       "      <th>...</th>\n",
       "      <th>zsigmond</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuehlke</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  0009f  007   05   10  100  1000     10000  100m   ...     \\\n",
       "0  0.0  0.0    0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   ...      \n",
       "1  0.0  0.0    0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   ...      \n",
       "2  0.0  0.0    0.0  0.0  0.0  0.0  0.0   0.0  0.010952   0.0   ...      \n",
       "3  0.0  0.0    0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   ...      \n",
       "4  0.0  0.0    0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   ...      \n",
       "\n",
       "   zsigmond  zucker  zuehlke  zuko  zukovsky  zulu  zundel  zurg  zwick  \\\n",
       "0       0.0     0.0      0.0   0.0       0.0   0.0     0.0   0.0    0.0   \n",
       "1       0.0     0.0      0.0   0.0       0.0   0.0     0.0   0.0    0.0   \n",
       "2       0.0     0.0      0.0   0.0       0.0   0.0     0.0   0.0    0.0   \n",
       "3       0.0     0.0      0.0   0.0       0.0   0.0     0.0   0.0    0.0   \n",
       "4       0.0     0.0      0.0   0.0       0.0   0.0     0.0   0.0    0.0   \n",
       "\n",
       "   zzzzzzz  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 34382 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=5)\n",
    "knc.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.17 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit knc.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=12345, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression(penalty='l2', fit_intercept=True, max_iter=100, C=1, solver=\"lbfgs\", random_state=12345)\n",
    "logr.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.07 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit logr.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 140 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit mnb.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_cnt = count_vec.transform(X_test).toarray()\n",
    "df_1 = pd.DataFrame(data=X_test_cnt, columns=count_vec.get_feature_names())\n",
    "df_1 = pd.DataFrame(data=transformer.transform(np.asarray(df_1)).toarray(),\n",
    "                       columns=count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yy = knc.predict(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 57.4 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit knc.predict(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438333333333\n",
      "0.561666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "print(mean_absolute_error(yy, y_test))\n",
    "print(accuracy_score(yy, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yy = logr.predict(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 22.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit logr.predict(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.271666666667\n",
      "0.728333333333\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(yy, y_test))\n",
    "print(accuracy_score(yy, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yy = mnb.predict(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 66.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit mnb.predict(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.345\n",
      "0.655\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(yy, y_test))\n",
    "print(accuracy_score(yy, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пояснения по результатам:\n",
    "\n",
    "Fitting time:\n",
    "* Дольше всего настраивается Логистическая регрессия, поскольку ей нужно настроить много параметров. (TODO: Expand)\n",
    "* Меньше всего времени на настройку тратит модель K Nearest Neighbours, поскольку она лишь запоминает выборку.\n",
    "\n",
    "Prediction time:\n",
    "На предсказании модели ведут себя противоположно своему поведению на настройке, т.е.:\n",
    "* Дольше всего предсказывает модель KNN, поскольку она должна померить расстояния евклидовой метрикой для каждого элемента из теста и каждого элемента из заученого трейна.\n",
    "* Меньше времени тратит Логистическая регрессия, поскольку (TODO: Expand)\n",
    "\n",
    "What about MNB? (TODO: Expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2.\n",
    "#### Применение k-folds (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>1. Повторите решение задачи 1, но вместо одного разделения на обучение и контроль используйте разбиение k-folds (k=4). Вам понадобится повторить все действия 4 раза. <br>\n",
    "2. Какой классификатор показывал лучшее/худшее качество на тестовой выборке? А при k-folds разбиении? Как вы думаете, обязательно ли в данной задаче оценивать качество на кросс-валидации, или достаточно отложить контрольную выборку и оценивать качество на ней?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 12345 # для всех объектов/методов/моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kfold(model, data, labels, k):\n",
    "    kfld = KFold(n_splits=k, random_state=12345)\n",
    "    y_pred = np.zeros(len(labels))\n",
    "    for i_train, i_test in kfld.split(data):\n",
    "        print(i_train)\n",
    "        X_train, X_test = data[i_train], data[i_test]\n",
    "        y_train = labels[i_train]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred[i_test] = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "#to reduce code :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "tmp = tfidfvec.fit_transform(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------KNN--------------------\n",
      "[ 350  351  352 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1047 1048 1049]\n",
      "0.614\n",
      "-------------------LR--------------------\n",
      "[ 350  351  352 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1047 1048 1049]\n",
      "0.8\n",
      "-------------------MNB--------------------\n",
      "[ 350  351  352 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1397 1398 1399]\n",
      "[   0    1    2 ..., 1047 1048 1049]\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print('-------------------KNN--------------------')\n",
    "kfold(knc, np.asarray(df_train), np.asarray(labels), 4)\n",
    "print('-------------------LR--------------------')\n",
    "kfold(logr, np.asarray(df_train), np.asarray(labels), 4)\n",
    "print('-------------------MNB--------------------')\n",
    "kfold(mnb, np.asarray(df_train), np.asarray(labels), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пояснения по результатам:\n",
    "\n",
    "KFold разбиение на 4 фолдах улучшело результат предсказания моделей, но если с KNN (при k = 5) улучшение заметное, то с Логистической регрессией и Наивным Байесом улучшения едва заметны.\n",
    "(WHY? TODO: Expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3.\n",
    "#### Выбор модели (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding-left:15px;\">1. Используя данные из задачи 1, разбейте обучающее подмножество (train) с использованием k-folds (k=4) <br>\n",
    "2. Рассмотрим следующие варианты значений гиперпараметров для наших классификаторов:  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>K-ближайших соседей</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = np.arange(1, 150, 20) # количество соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Логистическая регрессия</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs = np.logspace(-2, 10, 8, base=10) # параметр регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Мультиномиальный наивный Байес</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 1, 8, base=10) # сглаживающий параметр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfld=KFold(n_splits=4, random_state=12345)\n",
    "index = []\n",
    "for i_tr,i_tst in kfld.split(X_train):\n",
    "    index.append(i_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding-left:15px;\"> Найдите оптимальные значения гиперпараметров для классификаторов на кросс-валидации. Для этого постройте графики (гиперпараметр)-(качество) на обучении и валидации. <br> _Пояснение:_ вы разбили обучающую выборку на 4 блока. Для каждого значения гиперпараметра 4 раза повторите следующее: берем 3 блока для обучения, по ним настраиваем  TfIdf и обучаем классификатор, считаем качество на этих блоках (качество на обучении) и на оставшемся (качество на валидации). Итоговое значение качества на обучении для данного значения гиперпараметра - это среднее четырех полученных значений качества на обучении, то же самое с итоговым значением качества на валидации.  <br>\n",
    "3. 3 настроенные модели обучите на всем обучающем подмножестве (train) и протестируйте на тестовом (test). Определите время обучения и предсказания (см. задачу 1 п. 6)<br>\n",
    "4. Повторите шаги 2-4 для n-gram=2<br>\n",
    "5. Выведите итоговые данные по всем методам для лучших моделей (метод, n-gram, значение параметра модели, время обучения, время предсказания, доля правильных классификаций)<br>\n",
    "6. Сделайте выводы по полученным результатам: <ul>\n",
    "<li>какой метод показал наилучшее качество на обучении? на валидации? на тестовой выборке? Если это разные классификаторы, подумайте, почему так происходит. Если один и тот же, в чем его преимущества перед остальными?</li>\n",
    "<li>велика ли разница между качеством на обучении и на валидации? на валидации и контроле? Почему так происходит?</li>\n",
    "<li>что означает n-gram=2? Улучшилось ли качество при переходе от n-gram=1 к n-gram=2? Предложите свои идеи, почему.</li>\n",
    "<li>есть ли связь между качеством классификации и временем обучения/предсказания? какой классификатор обучается медленнее всего? медленнее всего делает предсказания? В чем причина?</li>\n",
    "</ul></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 12345 # для всех объектов/методов/моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparams_info(model, data, labels, index):\n",
    "    qual_train = []\n",
    "    qual_valid = []\n",
    "    for i_tr, i_tst in kfld.split(data):\n",
    "        tfidfvec = TfidfVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "        tmp = tfidfvec.fit_transform(data[i_tr])\n",
    "        model.fit(tmp, labels[i_tr])\n",
    "        qual_train.append(accuracy_score(model.predict(tmp), labels[i_tr]))\n",
    "        qual_valid.append(accuracy_score(model.predict(tfidfvec.transform(data[i_tst])), labels[i_tst]))\n",
    "        #print('-train indexes:', i_tr,\n",
    "        #      '-validation indexes: ', i_tst, \n",
    "        #      '-train accuracy score: ', qual_train[-1],\n",
    "        #      '-validation accuracy score: ', qual_valid[-1], sep='\\n')\n",
    "        #print()\n",
    "    print('---------------------result---------------------')\n",
    "    print('-mean train: ', np.mean(qual_train))\n",
    "    print('-mean validation: ', np.mean(qual_valid))\n",
    "    return np.mean(qual_train), np.mean(qual_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN n_neighbors= 1\n",
      "---------------------result---------------------\n",
      "-mean train:  0.999523809524\n",
      "-mean validation:  0.351428571429\n",
      "\n",
      "KNN n_neighbors= 21\n",
      "---------------------result---------------------\n",
      "-mean train:  0.729761904762\n",
      "-mean validation:  0.666428571429\n",
      "\n",
      "KNN n_neighbors= 41\n",
      "---------------------result---------------------\n",
      "-mean train:  0.715714285714\n",
      "-mean validation:  0.701428571429\n",
      "\n",
      "KNN n_neighbors= 61\n",
      "---------------------result---------------------\n",
      "-mean train:  0.715238095238\n",
      "-mean validation:  0.71\n",
      "\n",
      "KNN n_neighbors= 81\n",
      "---------------------result---------------------\n",
      "-mean train:  0.714761904762\n",
      "-mean validation:  0.714285714286\n",
      "\n",
      "KNN n_neighbors= 101\n",
      "---------------------result---------------------\n",
      "-mean train:  0.714285714286\n",
      "-mean validation:  0.714285714286\n",
      "\n",
      "KNN n_neighbors= 121\n",
      "---------------------result---------------------\n",
      "-mean train:  0.714285714286\n",
      "-mean validation:  0.714285714286\n",
      "\n",
      "KNN n_neighbors= 141\n",
      "---------------------result---------------------\n",
      "-mean train:  0.714285714286\n",
      "-mean validation:  0.714285714286\n"
     ]
    }
   ],
   "source": [
    "#KNearestNeighbors\n",
    "qual_train = [0]*len(ns)\n",
    "qual_valid = [0]*len(ns)\n",
    "\n",
    "j = 0\n",
    "for i in ns:\n",
    "    print('\\nKNN', 'n_neighbors=', i)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    qual_train[j], qual_valid[j] = hyperparams_info(knn, np.asarray(X_train), np.asarray(labels), index)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f1225f76a90>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHjCAYAAAA6x4aXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noS5JUpkw1CVJKhOGuiRJZeL/B7ergFIvTNAOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12259fe9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(figsize=(8,8))\n",
    "ax1 = plt.subplot()\n",
    "ax1.plot(ns, qual_train, label='Train')\n",
    "ax2 = plt.subplot()\n",
    "ax2.plot(ns, qual_valid, label='Validation')\n",
    "plt.legend(loc=1, ncol=1)\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При одном соседе модель просто выучила обучающую выборку и очень плохо отработала на валидации, что и ожидаемо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic regression C= 0.01\n",
      "[ 350  351  352 ..., 1397 1398 1399]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-774b0c08026b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logistic regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mhyperparams_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-76c8b8763c86>\u001b[0m in \u001b[0;36mhyperparams_info\u001b[0;34m(model, data, labels, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtfidfvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mqual_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mqual_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bobrg/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bobrg/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    873\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    874\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "for i in cs:\n",
    "    print()\n",
    "    print('Logistic regression', 'C=', i)\n",
    "    logr = LogisticRegression(C=i)\n",
    "    hyperparams_info(logr, np.asarray(X_train), np.asarray(labels), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MultinomialNB\n",
    "for i in alphas:\n",
    "    print()\n",
    "    print('MNB', 'alpha=', i)\n",
    "    mnb = MultinomialNB(alpha=i)\n",
    "    hyperparams_info(mnb, np.asarray(X_train), np.asarray(labels), index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[some info about choosing n-grams and cv](https://stats.stackexchange.com/questions/155483/estimating-the-best-length-of-n-gram) <br> </br>\n",
    "что означает n-gram=2?\n",
    "если n-gram=1 будет считать вероятность встретить определенное слово, то n-gram=2 будет парсить строку на посл-ти из двух слов.\n",
    "(перекрест в одно слово: … to be or not to be …\t=> …, to be, be or, or not, not to, to be, …)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 4. (опционально)\n",
    "#### Исследование влияния количества признаков FeatureHasher на качество классификации (+3 балла к сумме по всем ДЗ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Изучите, что такое feature hashing (достаточно разобаться с документацией sklearn) и кратко опишите. Как будет меняться качество классификации для обозначенных ранее методов при использовании FeatureHasher (или HashingVectorizer) из пакета sklearn перед TF-IDF преобразованием, если</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = np.logspace(1, 5, 5, base=10) # количество признаков\n",
    "non_negative=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>В этом задании можно воспользоваться GridSearchCV</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задача 5. (опционально)\n",
    "__Простой прототип (+ 2 балла к сумме по всем ДЗ)__\n",
    "\n",
    "Напишите функцию, которая берет на вход произвольную строку и возвращает для нее предсказание для вашей задачи. Придумайте по 3 примера строк для положительного и отрицательного класса, сделайте для них предсказание. Совпадают ли ваши метки и предсказания классификатора? Оцените (любым способом), насколько придуманные вами тексты похожи на объекты датасета, с которым вы работали.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_class_for_text(s):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
