{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#basic\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#for text-preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#featureengineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#neuralnets\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout,Input, Concatenate, Convolution1D, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import SimpleRNN, LSTM, Embedding\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import RMSprop\n",
    "#embedings\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>999977655955</td>\n",
       "      <td>\"\\nI have discussed it, unlike most of those w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95847</th>\n",
       "      <td>999982426659</td>\n",
       "      <td>ps. Almost forgot, Paine don't reply back to t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95848</th>\n",
       "      <td>999982764066</td>\n",
       "      <td>Mamoun Darkazanli\\nFor some reason I am unable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95849</th>\n",
       "      <td>999986890563</td>\n",
       "      <td>Salafi would be a better term. It is more poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95850</th>\n",
       "      <td>999988164717</td>\n",
       "      <td>making wikipedia a better and more inviting pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "95846  999977655955  \"\\nI have discussed it, unlike most of those w...      0   \n",
       "95847  999982426659  ps. Almost forgot, Paine don't reply back to t...      1   \n",
       "95848  999982764066  Mamoun Darkazanli\\nFor some reason I am unable...      0   \n",
       "95849  999986890563  Salafi would be a better term. It is more poli...      0   \n",
       "95850  999988164717  making wikipedia a better and more inviting pl...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \n",
       "95846             0        0       0       0              0  \n",
       "95847             0        1       0       0              0  \n",
       "95848             0        0       0       0              0  \n",
       "95849             0        0       0       0              0  \n",
       "95850             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nI have discussed it, unlike most of those who revert me (Heonsi is a pure sockpuppet, Caden and Central Data Bank haven\\'t discussed at all). I think any unbiased party reading the talk page will see that Rjensen and R41 have been thoroughly refuted and now resort only to the classic pattern of merely repeating their long-countered points as if they were new (e.g. trotting out and even further adding to their meaningless \"\"sources\"\" for what is inherently an opinion).   \"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[95846]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ps. Almost forgot, Paine don't reply back to this shit, I don't want to see/care what you have to say do your bitching out of my sight, plskthxbai.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[95847]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train nulls\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n",
      "test nulls\n",
      "id              0\n",
      "comment_text    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print( 'train nulls',\n",
    "train.isnull().sum(),\n",
    "      'test nulls',\n",
    "test.isnull().sum(),\n",
    "sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.comment_text.fillna('unknown', inplace=True)\n",
    "test.comment_text.fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_ = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beautify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Amount of comments in ds')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAG5CAYAAAADNAT0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dae4d4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = train[classes_].sum(axis=1).unique()\n",
    "bins.sort()\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.hist(train[classes_].sum(axis=1), bins)\n",
    "plt.title('Labels info_1')\n",
    "plt.xlabel('Amount of classes for comment')\n",
    "plt.ylabel('Amount of comments in ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            9237\n",
       "severe_toxic      965\n",
       "obscene          5109\n",
       "threat            305\n",
       "insult           4765\n",
       "identity_hate     814\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[classes_].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS TOXIC\n",
      "ps. Almost forgot, Paine don't reply back to this shit, I don't want to see/care what you have to say do your bitching out of my sight, plskthxbai.\n",
      "\n",
      "CLASS SEVERE_TOXIC\n",
      "98.248.32.178 I will set you on fire, I will shoot your ass up. I will cut your penis off and I will shove it down your throat and choke you. I will cut you up big time motherfucker.\n",
      "\n",
      "CLASS OBSCENE\n",
      "ps. Almost forgot, Paine don't reply back to this shit, I don't want to see/care what you have to say do your bitching out of my sight, plskthxbai.\n",
      "\n",
      "CLASS THREAT\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "SuPeRTR0LL WiLL LiVe FoReVeR!\n",
      "iF You DoN'T ReSPeCT THe SuPeRTR0LL You WiLL Die You PaTHeTiC FooL!\n",
      "\n",
      "CLASS INSULT\n",
      "asshole and Richard Simmons's asshole, and assisted in the development of a faggoty article on the jizzmaster himself, me, (the latter of which has since undergone faggoty tranny surgery and a tit job. Further revisions to my cock include trying to fuck my mother with it, which I am not entirely happy about, but heck, it's a mom I remind you all I am a faggot who wouldn't risk pissing with the seat down, so you can tell my nickname is a faggoty attempt to appear edgy. I weigh 450kg..\n",
      "\n",
      "CLASS IDENTITY_HATE\n",
      "so, who the fuck gives a damn, you dipshitted lesbian?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in classes_:\n",
    "    print('CLASS', i.upper())\n",
    "    tmp = np.asarray(train[train[i] == 1].comment_text)\n",
    "    print(tmp[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comments contain IP-addresses, user names, urls and a lot of other mess. (better to clean up with regex, etc)\n",
    "* Also some comments contain lots of repetitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to add amount of negative words as feature. Here lies dictionary: https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/blob/master/data/opinion-lexicon-English/negative-words.txt\n",
    "<br> Expanded dict-ry with more offensive words from urban slangs\n",
    "<br> Added more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_words = [i[:-1] for i in open('negative-words.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_words_counts = train.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in neg_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_words_counts_ts = test.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in neg_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbs = ['!', '@', '#', '$', '%', '^', '&', '(', '*', '№', '=', '-', '+', '<', '>']\n",
    "punct = ['.', ',', '\\'', ';', ':']\n",
    "smailes = [':)', ':D', ')']\n",
    "neg_phrases = ['fuck you', 'screw you', 'kill you', 'beat you', 'die you', 'damn it', 'damn you', 'bitchin out', 'duck face'  ]\n",
    "short_phrases = ['wtf', 'dgaf', 'fml', 'foad', 'ftw', 'milf']\n",
    "races_and_orients = ['lesbian', 'gay', 'homo', 'bromance', 'dyke', 'lesbo', 'lezzie', 'faggot', 'nigga', 'nigger', 'jew', 'yankee', 'racist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbols_count = train.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in symbs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punct_count = train.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smailes_count = train.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in smailes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_of_comment = train.comment_text.apply(lambda a: len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capital_letters = train.comment_text.apply(lambda a: len([i for i in a if i.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_phrases_count = train.comment_text.apply(lambda a: len([i for i in neg_phrases if i in a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_phrases_count = train.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in short_phrases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "races_and_orients_count = train.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in races_and_orients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_spaces = train.comment_text.apply(lambda a: len([i for i in a if i == ' ']))\n",
    "count_ips = train.comment_text.apply(lambda a: len(re.findall(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\", a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capital_letters_ts = test.comment_text.apply(lambda a: len([i for i in a if i.isupper()]))\n",
    "len_of_comment_ts = test.comment_text.apply(lambda a: len(a))\n",
    "count_spaces_ts = test.comment_text.apply(lambda a: len([i for i in a if i == ' ']))\n",
    "count_ips_ts = test.comment_text.apply(lambda a: len(re.findall(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\", a)))\n",
    "punct_count_ts = test.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in punct]))\n",
    "symbols_count_ts = test.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in symbs]))\n",
    "smailes_count_ts = test.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in smailes]))\n",
    "neg_phrases_count_ts = test.comment_text.apply(lambda a: len([i for i in neg_phrases if i in a]))\n",
    "short_phrases_count_ts = test.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in short_phrases]))\n",
    "races_and_orients_count_ts = test.comment_text.apply(lambda a: len([i for i in a.lower().split() if i in races_and_orients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.assign(capital_letters=capital_letters,\n",
    "                     len_of_comment=len_of_comment,\n",
    "                     count_ips=count_ips,\n",
    "                     count_spaces=count_spaces,\n",
    "                     sybs = symbols_count,\n",
    "                     smailes = smailes_count,\n",
    "                     neg_words=neg_words_counts,\n",
    "                    neg_phrases=neg_phrases_count,\n",
    "                    short_phrases=short_phrases_count,\n",
    "                    races_and_orients=races_and_orients_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.assign(capital_letters=capital_letters_ts,\n",
    "                     len_of_comment=len_of_comment_ts,\n",
    "                     count_ips=count_ips_ts,\n",
    "                     count_spaces=count_spaces_ts,\n",
    "                     sybs = symbols_count_ts,\n",
    "                     smailes = smailes_count_ts,\n",
    "                     neg_words=neg_words_counts_ts,\n",
    "                    neg_phrases=neg_phrases_count_ts,\n",
    "                    short_phrases=short_phrases_count_ts,\n",
    "                    races_and_orients=races_and_orients_count_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['capital_letters', 'len_of_comment', 'count_ips', 'count_spaces', 'symbs', 'smailes', 'neg_words', 'neg_phrases', 'short_phrases', 'races_and_orients']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizing with TweetTokenizer NLTK. (seem to be cool tool:D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def beautify_text(s):\n",
    "    s = s.replace(':)', 'smile')\n",
    "    s = s.replace(':D', 'laugh')\n",
    "    # Removes all characters from string except letters and digits and convert letters to lowercase\n",
    "    s = re.sub(\"[^a-zA-Z0-9]\", \" \", s.lower())\n",
    "    # Remove new lines\n",
    "    s = re.sub(\"\\\\n\",\"\", s)\n",
    "    # Change urls\n",
    "    s = re.sub(\"[a-zA-Z0-9]*(https://)[a-zA-Z0-9.]*\", \"url\", s)\n",
    "    # Remove IPs\n",
    "    s = re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\"\", s)\n",
    "    # Remove usernames\n",
    "    s = re.sub(\"\\[\\[.*\\]\",\"\", s)\n",
    "    # Remove numbers\n",
    "    s = re.sub(\"\\\\b[0-9]+\\\\b\", \"\", s)\n",
    "    # better use (\\w+[0-9])\n",
    "    # fix grammar\n",
    "    s = re.sub('\\\\bf{1,}[a-z]*(ck)\\\\b', 'fuck', s)\n",
    "    # find cock symb lol\n",
    "    s = re.sub('8={1,}D?', 'cock', s)\n",
    "\n",
    "    #formalize text\n",
    "    s = s.replace(\"don\\'t\", \"do not\")\n",
    "    s = s.replace(\"doesn\\'t\", \"does not\")\n",
    "    s = s.replace(\"didn\\'t\", \"did not\")\n",
    "    s = s.replace(\"won\\'t\", \"will not\")\n",
    "    s = s.replace(\"haven\\'t\", \"have not\")\n",
    "    s = s.replace('haven\\\\\\'t', 'have not')\n",
    "    #s = s.replace('\\'ll', ' will')\n",
    "    #clean text\n",
    "    s = s.replace('\\\\', ' ')\n",
    "    s = s.replace('\\n', '')\n",
    "    s = s.replace(\"'s\", ' is')\n",
    "    s = s.replace('“', '')\n",
    "    s = s.replace('fck', 'fuck')\n",
    "    s = s.replace('wtf', 'what the fuck')\n",
    "    s = s.replace('cockwad', 'idiot')\n",
    "    s = s.replace('скоморохъ', 'buffoon')\n",
    "    s = s.replace('backash', 'backlash')\n",
    "    s = s.replace('bizarrethere', 'bizarre there')\n",
    "    s = s.replace('backash', 'backlash')\n",
    "    s = s.replace('dummasses', 'dumbass')\n",
    "    s = s.replace('lesbianswomen', 'lesbian women')\n",
    "    s = s.replace('bisexualis', 'bisexual')\n",
    "    s = s.replace('sexualitypenis','sexuality penis')\n",
    "    s = s.replace('dorkgasm', 'orgasm')\n",
    "    s = s.replace('cockmy', 'cock my')\n",
    "    s = s.replace('supertroll', 'super troll')\n",
    "    s = s.replace('faggotttttttt', 'faggot')\n",
    "    s = s.replace('sucks50', 'sucks 50')\n",
    "    s = s.replace('themfuck', 'them fuck')\n",
    "    s = s.replace('offenderfugitive', 'offender fugitive')\n",
    "    s = s.replace('creationisrael', 'creation israel')\n",
    "    s = s.replace('assholestating', 'asshole stating')\n",
    "    s = s.replace('idiotenough', 'idiot enough')\n",
    "    s = s.replace('lessblunt', 'less blunt')\n",
    "    s = s.replace('anticanadian', 'anti canadian')\n",
    "    s = s.replace('afrikanblack', 'african black')\n",
    "    s = s.replace('againbitch', 'again bitch')\n",
    "    s = s.replace('animalfucker', 'animal fucker')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-1a4d3681756f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbeautiful_text_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeautify_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbeautiful_text_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeautify_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/bobrg/anaconda3/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-f36d54e85826>\u001b[0m in \u001b[0;36mbeautify_text\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Change urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[a-zA-Z0-9]*(https://)[a-zA-Z0-9.]*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Remove IPs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bobrg/anaconda3/lib/python3.5/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beautiful_text_tr = train.comment_text.apply(beautify_text)\n",
    "beautiful_text_ts = test.comment_text.apply(beautify_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beautiful_text_ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ce4b514c3461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwords_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeautiful_text_tr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeautiful_text_ts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'beautiful_text_ts' is not defined"
     ]
    }
   ],
   "source": [
    "words_tr = [tt.tokenize(i) for i in beautiful_text_tr]\n",
    "words_ts = [tt.tokenize(i) for i in beautiful_text_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tr = [[wnl.lemmatize(j) for j in i] for i in words_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_ts = [[wnl.lemmatize(j) for j in i] for i in words_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tr = [[i for i in j if i not in stopwords.words('english')] for j in words_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_ts = [[i for i in j if i not in stopwords.words('english')] for j in words_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train_new = [' '.join(i) for i in words_tr]\n",
    "text_test_new = [' '.join(i) for i in words_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('train_preproc', words_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('test_preproc', words_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tr = np.load('train_preproc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_ts = np.load('test_preproc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train_new = [' '.join(i) for i in words_tr]\n",
    "text_test_new = [' '.join(i) for i in words_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF using n_gram from 1 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=50,  max_features=30000, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(1,3),\n",
    "            use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = tfv.fit_transform(text_train_new)\n",
    "test_ = tfv.transform(text_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = pad_sequences(train_.toarray(), maxlen=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ = pad_sequences(test_.toarray(), maxlen=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For embedings I'm gonna use *bag-of-means* (https://arxiv.org/pdf/1509.01626.pdf)<br>\n",
    "<p>\"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset, and then use these learnt means as representatives of the clustered words. We take into consideration all the words that appeared more than 5 times in the training subset. The bag-of-means features are computed the same way as in the bag-of-words model. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(words_tr, min_count=100, size=500)\n",
    "word_vectors = model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=2800)\n",
    "idx = kmeans_model.fit_predict(word_vectors)\n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fuck', 1.0),\n",
       " ('retard', 0.6874533891677856),\n",
       " ('asshol', 0.6376489400863647),\n",
       " ('motherfuck', 0.617753267288208),\n",
       " ('cunt', 0.6025792956352234),\n",
       " ('garbag', 0.5999402403831482),\n",
       " ('bitch', 0.5947405695915222),\n",
       " ('shut', 0.5876146554946899),\n",
       " ('shit', 0.5849085450172424),\n",
       " ('damn', 0.582467257976532)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(kmeans_model.cluster_centers_[540])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_dim = 500\n",
    "embedding_matrix = np.zeros((len(kmeans_model.cluster_centers_), vector_dim))\n",
    "for i in range(len(kmeans_model.cluster_centers_)):\n",
    "    embedding_vector = kmeans_model.cluster_centers_[i]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea didn't match the intetions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GloVe Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "EMBEDDING_DIM = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b5fef4ab3599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./glove-2/glove.twitter.27B.200d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('./glove-2/glove.twitter.27B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_train_new)\n",
    "\n",
    "sequences_tr = tokenizer.texts_to_sequences(text_train_new)\n",
    "sequences_ts = tokenizer.texts_to_sequences(text_test_new)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('émbed_matrix', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('émbed_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train ,y_eval = train_test_split(train_, train[classes_].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_LSTM():\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(Embedding(embedding_matrix.shape[0], 200, weights=[embedding_matrix]))\n",
    "    model_lstm.add(Bidirectional(LSTM(150, recurrent_dropout=0.25 , return_sequences=True)))\n",
    "    model_lstm.add(GlobalMaxPool1D())\n",
    "    model_lstm.add(Dropout(0.25))\n",
    "    model_lstm.add(Dense(50, activation=\"relu\"))\n",
    "    model_lstm.add(Dropout(0.25))\n",
    "    model_lstm.add(Dense(6, activation=\"sigmoid\"))\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])    \n",
    "\n",
    "\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1 = model_LSTM()\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_loss:.6f}.hdf5', \n",
    "                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=4,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(ind):\n",
    "    a = [0.001, 0.001, 0.0001, 0.0001, 0.00001, 0.00001, 0.000001]\n",
    "    return a[ind]\n",
    "lr = LearningRateScheduler(schedule)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76680, 8636)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76680 samples, validate on 19171 samples\n",
      "Epoch 1/10\n",
      "76672/76680 [============================>.] - ETA: 1s - loss: 0.1510 - acc: 0.9631Epoch 00001: val_loss improved from inf to 0.14134, saving model to weights.001-0.141338.hdf5\n",
      "76680/76680 [==============================] - 11528s 150ms/step - loss: 0.1510 - acc: 0.9631 - val_loss: 0.1413 - val_acc: 0.9632\n",
      "Epoch 2/10\n",
      "76672/76680 [============================>.] - ETA: 1s - loss: 0.1452 - acc: 0.9631Epoch 00002: val_loss improved from 0.14134 to 0.14127, saving model to weights.002-0.141269.hdf5\n",
      "76680/76680 [==============================] - 11821s 154ms/step - loss: 0.1452 - acc: 0.9631 - val_loss: 0.1413 - val_acc: 0.9632\n",
      "Epoch 3/10\n",
      "76672/76680 [============================>.] - ETA: 1s - loss: 0.1443 - acc: 0.9631Epoch 00003: val_loss improved from 0.14127 to 0.14116, saving model to weights.003-0.141157.hdf5\n",
      "76680/76680 [==============================] - 11845s 154ms/step - loss: 0.1443 - acc: 0.9631 - val_loss: 0.1412 - val_acc: 0.9632\n",
      "Epoch 4/10\n",
      "42048/76680 [===============>..............] - ETA: 1:24:01 - loss: 0.1447 - acc: 0.9629"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-6c65fd28b7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_1.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_eval, y_eval),\n\u001b[0;32m----> 2\u001b[0;31m             callbacks=[early_stop, checkpoint, lr])\n\u001b[0m",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gleb/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_eval, y_eval),\n",
    "            callbacks=[early_stop, checkpoint, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model_1.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30001431693307462"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_eval, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.append(test.id.values.reshape(-1,1), model_1.predict(test_), axis=1), columns=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "df.id = df.id.apply(lambda a: int(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('predictions', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's first try Conv+LSTM NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "* Using *sigmoid* on last layer instead of *softmax* 'cause we want probability of each class. So we are using sigmoid on final layer, which gives output in range 0 to 1. If our aim was to find the class, then we will have used softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "comment_text (InputLayer)       (None, 1500)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1500, 200)    24168800    comment_text[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "numeric (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)        (None, 150)          52650       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 155)          0           numeric[0][0]                    \n",
      "                                                                 simple_rnn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 150)          23400       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           7550        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            306         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,252,706\n",
      "Trainable params: 24,252,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "embeddings = []\n",
    "    \n",
    "input_text = Input((1500,), name = 'comment_text')\n",
    "input_num = Input((5, ), name = 'numeric')\n",
    "\n",
    "embedding_text = Embedding(embedding_matrix.shape[0], 200, weights=[embedding_matrix])(input_text)\n",
    "\n",
    "lstm_layer = (SimpleRNN(150))(embedding_text)\n",
    "\n",
    "_features_ = [input_num, lstm_layer]\n",
    "net = Concatenate()(_features_)\n",
    "\n",
    "net = Dense(150, kernel_initializer = 'he_normal', activation='relu')(net)\n",
    "net = Dense(50, kernel_initializer = 'he_normal', activation='relu')(net)\n",
    "\n",
    "output = Dense(6, activation=\"sigmoid\")(net)\n",
    "\n",
    "model = Model([input_num, input_text], output)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_num_scaled = scaler.fit_transform(train[features])\n",
    "tr = [x_num_scaled[:len(X_train)], X_train]\n",
    "val = [x_num_scaled[len(X_train):], X_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights_srnn.{epoch:03d}-{val_loss:.6f}.hdf5', \n",
    "                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=4,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4)\n",
    "def schedule(ind):\n",
    "    a = [0.001, 0.001, 0.0001, 0.0001, 0.00001, 0.00001, 0.000001]\n",
    "    return a[ind]\n",
    "lr = LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76680, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76680 samples, validate on 19171 samples\n",
      "Epoch 1/6\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9631Epoch 00001: val_loss improved from inf to 0.14165, saving model to weights_srnn.001-0.141655.hdf5\n",
      "76680/76680 [==============================] - 1727s 23ms/step - loss: 0.1430 - acc: 0.9631 - val_loss: 0.1417 - val_acc: 0.9632\n",
      "Epoch 2/6\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9631Epoch 00002: val_loss did not improve\n",
      "76680/76680 [==============================] - 2107s 27ms/step - loss: 0.1427 - acc: 0.9631 - val_loss: 0.1418 - val_acc: 0.9632\n",
      "Epoch 3/6\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9631Epoch 00003: val_loss improved from 0.14165 to 0.14126, saving model to weights_srnn.003-0.141256.hdf5\n",
      "76680/76680 [==============================] - 2815s 37ms/step - loss: 0.1418 - acc: 0.9631 - val_loss: 0.1413 - val_acc: 0.9632\n",
      "Epoch 4/6\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9631Epoch 00004: val_loss improved from 0.14126 to 0.14122, saving model to weights_srnn.004-0.141217.hdf5\n",
      "76680/76680 [==============================] - 1978s 26ms/step - loss: 0.1418 - acc: 0.9631 - val_loss: 0.1412 - val_acc: 0.9632\n",
      "Epoch 5/6\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9631Epoch 00005: val_loss did not improve\n",
      "76680/76680 [==============================] - 1582s 21ms/step - loss: 0.1417 - acc: 0.9631 - val_loss: 0.1412 - val_acc: 0.9632\n",
      "Epoch 6/6\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9631Epoch 00006: val_loss improved from 0.14122 to 0.14120, saving model to weights_srnn.006-0.141200.hdf5\n",
      "76680/76680 [==============================] - 1582s 21ms/step - loss: 0.1417 - acc: 0.9631 - val_loss: 0.1412 - val_acc: 0.9632\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tr, y_train, batch_size=64, epochs = 6, validation_data=(val, y_eval), \n",
    "              callbacks=[lr, early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_num_scaled_ts = scaler.fit_transform(test[features])\n",
    "ts = [x_num_scaled_ts, test_]\n",
    "\n",
    "df = pd.DataFrame(np.append(test.id.values.reshape(-1,1), model.predict(ts), axis=1), columns=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "df.id = df.id.apply(lambda a: int(a))\n",
    "df.to_csv('submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "embeddings = []\n",
    "    \n",
    "input_text = Input((1500,), name = 'comment_text')\n",
    "input_num = Input((5, ), name = 'numeric')\n",
    "\n",
    "embedding_text = Embedding(embedding_matrix.shape[0], 200, weights=[embedding_matrix])(input_text)\n",
    "\n",
    "lstm_layer = (ConvLSTM2D(filters=150, kernel_size=3))(embedding_text)\n",
    "\n",
    "_features_ = [input_num, lstm_layer]\n",
    "net = Concatenate()(_features_)\n",
    "\n",
    "net = Dense(150, kernel_initializer = 'he_normal', activation='relu')(net)\n",
    "net = Dense(50, kernel_initializer = 'he_normal', activation='relu')(net)\n",
    "\n",
    "output = Dense(6, activation=\"sigmoid\")(net)\n",
    "\n",
    "model = Model([input_num, input_text], output)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_num_scaled = scaler.fit_transform(train[features])\n",
    "tr = [x_num_scaled[:len(X_train)], X_train]\n",
    "val = [x_num_scaled[len(X_train):], X_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights_cnn.{epoch:03d}-{val_loss:.6f}.hdf5', \n",
    "                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=4,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4)\n",
    "def schedule(ind):\n",
    "    a = [0.001, 0.001, 0.0001, 0.0001, 0.00001, 0.00001, 0.000001]\n",
    "    return a[ind]\n",
    "lr = LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(tr, y_train, batch_size=64, epochs = 6, validation_data=(val, y_eval), \n",
    "              callbacks=[lr, early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.append(test.id.values.reshape(-1,1), model.predict(ts), axis=1), columns=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "df.id = df.id.apply(lambda a: int(a))\n",
    "df.to_csv('submission_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
